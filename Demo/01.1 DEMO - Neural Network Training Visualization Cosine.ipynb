{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a9bcc4941471f15",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Visualizing Training Process of Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a22380766c08eb",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "* by: [Jannik Mettner](mailto:jmettner@stud.hs-heilbronn.de)\n",
    "* Heilbronn University, Germany\n",
    "* Created: March 2024,  Last Edited: March 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d247a9aed141",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "In this notebook I will visualize the training process of a basic neural network that will learn to approximate the cosine function. By the end of the notebook you should be able to:\n",
    "* create a pytorch dataset\n",
    "* create a basic pytorch neural network \n",
    "* implement the main loop for training the network on the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ad4b3fd9e4d99",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 0 - Basic imports\n",
    "If you want to run pytorch with your gpu, follow this guide: https://pytorch.org/get-started/locally/\n",
    "Supported are cuda enabled GPUs from Nvidia and Apple Silicon GPUs. To use your Nvidia GPU click the highest CUDA version in the guide and to use Apple Silicon make sure to select the Preview (Nightly) build, otherwise only the CPU is used. Using anything but Conda has lead to a erroneous installation for me and Conda is the officially recommended way to install pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "#!pip install numpy torch matplotlib Pillow torchvision tqdm ipython scikit-learn torchview graphviz\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import torch\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from torchview import draw_graph\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "if torch.cuda.device_count() > 0: # Check if Nvidia GPU is used\n",
    "    device = \"cuda\"\n",
    "    print(\"The device used for training is\", torch.cuda.get_device_name(0))\n",
    "elif torch.backends.mps.is_available(): # Check if Apple Silicon GPU is used\n",
    "    device = \"mps\"\n",
    "    x=torch.ones(1).to(device)\n",
    "    print(\"The device used for training is Apple Silicon GPU\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"The device used for training is the cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6beaa4e2df7cdd2",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 1 - The Dataset\n",
    "Neural networks are considered to be universal function approximators. Now we want to approximate the simple cosine function  $ y=cos(x) $ | $ f: \\mathbb{R} \\rightarrow [-1,1] $ . \n",
    "First of all, we define the dataset that is used for training the neural net. It consists of three main functions that should be defined. The first is the __init__ function, that creates the dataset object and stores important variables like the datapoints and the scaler used for the datapoints. The second function simply returns the length of the dataset and the third function returns the x and y values of the datapoint at position $ \\texttt{idx} $\n",
    "The cosine function is defined for an infinite amount of inputs and outputs but the dataset for training a neural network has to be finite. Therefore, the function will be sampled in the interval $ [ \\texttt{min} , \\texttt{max} ] $ with $ \\texttt{n_datapoints} $ equally spaced datapoints inbetween.\n",
    "Since $ y $ is already in the range of -1 to 1, it does not have to be rescaled. However, x is in the range of min to max and for efficiently training the neural network, we normalize it to the range -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbf8f1067728fa7",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T18:57:15.108686400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def function(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "class CosineDataset(Dataset):\n",
    "    def __init__(self, n_datapoints, min, max):\n",
    "        np.random.seed(1)\n",
    "        self.x_values = np.random.uniform(min, max, n_datapoints)\n",
    "        self.x_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        \n",
    "        self.y_values = function(self.x_values)\n",
    "        self.y_scaler = MinMaxScaler(feature_range=(-1,1))\n",
    "        \n",
    "        self.x_values = self.x_scaler.fit_transform(self.x_values.reshape(-1,1))\n",
    "        self.y_values = self.y_scaler.fit_transform(self.y_values.reshape(-1,1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x_values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x_values[idx]\n",
    "        y = self.y_values[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cb6bfa82b1a835",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T18:57:15.109686800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the dataset object\n",
    "dataset = CosineDataset(n_datapoints=100, min=0, max=40)\n",
    "\n",
    "# Now a dataloader is defined that uses the previously defined dataset to pull multiple datapoints simultaneously from the dataset to create a mini-batch.\n",
    "# The size of the mini-batches has to be specified for that. Also, datapoints are shuffled to make the model train more efficiently.\n",
    "batch_size = 32\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "for x, y in loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5077eb2492cae5cd",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's visualize the dataset by looping over it and storing all values for x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533967059e066b8c",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T18:57:15.111686400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "points = np.array([[x, y] for x, y in dataset])\n",
    "# Visualize the data as it is seen by the neural network\n",
    "plt.scatter(points[:,0], points[:,1])\n",
    "plt.ylabel(\"function(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-5,5)\n",
    "plt.title(\"Neural network's view on the data\")\n",
    "plt.show()\n",
    "# Visualize the data with the correct and unscaled values for x.\n",
    "plt.scatter(dataset.x_scaler.inverse_transform(points[:,0].reshape(-1,1)), dataset.y_scaler.inverse_transform(points[:,1].reshape(-1,1)) ) # undo scaling with scaler.inverse_transform\n",
    "plt.ylabel(\"function(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylim(-5,5)\n",
    "plt.title(\"Unscaled view on the data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558eff761c62518",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2 - The Neural Network\n",
    "Now we will define the neural network that should solve the task. Feel free to play around with the layers and hyperparameters and observe their effects on the training output.\n",
    "To create the neural network, only two functions have to be defined. The __init__ function simply initializes and stores all the layers given their hyperparameters and the forward function defines the order in which a datapoint passes through the layers.\n",
    "For this task we create 4 fully connected / dense / linear layers. The important thing here is to keep track of the shape/size/features of datapoints that go in and out of each layer. Since we want to approximate the cosine function that takes one number and outputs one number, the first layers $\\texttt{in_features}$ is 1 and the last layers $\\texttt{out_features}$ is 1. For the hidden layers, it is only important that the $\\texttt{out_features}$ of the previous layer is the same as the $\\texttt{in_features}$ of the current.\n",
    "In the forward function the input x passes through the previously defined dense layers with the activation function $\\texttt{leaky_relu}$ inbetween the hidden layers. The last function applied is the $\\texttt{tanh}$ function which makes sure that the value y can only be between -1 and 1 as it should because the cosinus function can also only output values between -1 and 1. If the sigmoid function would be used instead, the values of y would be in the range 0 to 1 which makes it impossible to approximate the cosine function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aa2fefe3a0bcd2",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T18:57:15.112685700Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel,self).__init__()\n",
    "        self.linear1 = nn.Linear(in_features=1, out_features=100)\n",
    "        self.linear2 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.linear3 = nn.Linear(in_features=100, out_features=100)\n",
    "        self.linear4 = nn.Linear(in_features=100, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        block1 = F.leaky_relu(self.linear1(x))\n",
    "        block2 = F.leaky_relu(self.linear2(block1))\n",
    "        block3 = F.leaky_relu(self.linear3(block2))\n",
    "        y = F.tanh(self.linear4(block3))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20744854f965a8e3",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-18T18:57:15.113686200Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# If you want to visualize the model, you need to install graphviz to your system. Otherwise, comment those two lines.\n",
    "# Windows: winget install graphviz          And then add C:\\Program Files\\Graphviz\\bin to the environment variables and restart the ide\n",
    "# MacOS: brew install graphviz\n",
    "# Unix: sudo apt-get install graphviz \n",
    "# The input size is (batch_size, x).\n",
    "model_graph = draw_graph(model, input_size=(32,1), expand_nested=False)\n",
    "model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbff645d8698951",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 3 - Training the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84b4d9023e8a864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-18T18:57:15.114686400Z",
     "start_time": "2024-03-18T18:57:15.114686400Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device) # move the model to the gpu, if one exists\n",
    "# Create the loss function and optimizer\n",
    "loss_func = nn.MSELoss() # Define the loss function. The Mean Squared Error can be used for such a regression task.\n",
    "\n",
    "# Define the optimizer. One of the first ones was stochastic gradient descent but the Adam optimizer is now the most widely used one.\n",
    "# The learning rate (lr) is also specified here.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) \n",
    "\n",
    "\n",
    "# Preparing the plot that visualizes the training process \n",
    "fig, ax = plt.subplots(2,1, figsize=(10,10))\n",
    "\n",
    "num_epochs = 1000\n",
    "losses = []\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0 # To determine the average loss of all datapoints in that epoch\n",
    "    \n",
    "    # Visualizes in the jupyter output the loop over the dataloader. Shows what the current epoch, mini-batch and MSE is.\n",
    "    progress_bar = tqdm(loader, desc=f'Epoch {epoch+1}/{num_epochs}') \n",
    "    for i, (x, y) in enumerate(progress_bar):\n",
    "        # Bring data into the right shape and move it to the gpu, if available\n",
    "        x, y = x.to(torch.float32).unsqueeze(1).to(device), y.to(torch.float32).unsqueeze(1).to(device) \n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(y_pred, y)\n",
    "        epoch_loss += loss.item() * x.size(0) # Mean loss of the batch times the number of datapoints in the batch to get the accumulated epoch loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad() # Reset gradients to 0 so the next backward() call can add gradients of this batch\n",
    "        loss.backward() # Backpropagation\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.01) # The gradients for each parameter is stored together with the parameter. We clip the gradients so they do not explode.\n",
    "        \n",
    "        optimizer.step() # The optimizer goes one step into the optimal direction on the error surface to reduce the error.\n",
    "        \n",
    "        progress_bar.set_postfix(MSE=epoch_loss / ((i*batch_size)+1)) # Update the jupyter progress bar to include the current loss\n",
    "    losses.append(epoch_loss/len(loader.dataset))\n",
    "    \n",
    "    \n",
    "    # Updating the figure to visualize current prediction accuracy of the neural net\n",
    "    ax[0].cla()\n",
    "    ax[1].cla()\n",
    "    display.clear_output(wait=True)\n",
    "    ax[0].scatter(dataset.x_scaler.inverse_transform(points[:,0].reshape(-1,1)), points[:,1], label=\"Training points\")\n",
    "    x_values = np.linspace(-1.3,1.3,200) \n",
    "    outputs = model(torch.tensor(x_values).to(torch.float32).unsqueeze(1).to(device)).cpu().detach().numpy()\n",
    "    ax[0].plot(dataset.x_scaler.inverse_transform(x_values.reshape(-1,1)), outputs, c=\"red\", label=\"Prediction\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Neural network predicting the cosine function\")\n",
    "    ax[0].set_ylim([-2.5,2.5])\n",
    "    ax[0].set_ylabel(\"function(x)\")\n",
    "    ax[0].set_xlabel(\"x\")\n",
    "    ax[1].set_xlim([0, num_epochs])\n",
    "    ax[1].set_ylim([0, 1])\n",
    "    ax[1].plot(losses, label=\"MSE\", c=\"red\")\n",
    "    ax[1].set_title(\"Mean Squared Error\")\n",
    "    ax[1].set_ylabel(\"MSE\")\n",
    "    ax[1].set_xlabel(\"Epoch\")\n",
    "    ax[1].legend()\n",
    "    display.display(fig)\n",
    "\n",
    "    # Log the average loss per epoch\n",
    "    print(f'Epoch {epoch+1}, Average Loss: {epoch_loss/len(loader.dataset):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
