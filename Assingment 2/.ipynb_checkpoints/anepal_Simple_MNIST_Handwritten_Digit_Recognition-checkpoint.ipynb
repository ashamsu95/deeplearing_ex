{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84nsAk3z1Pry"
   },
   "source": [
    "# **Handwritting Digit Recognition**\n",
    "by Aashamsu Nepal for deep learing course Assigment 2\n",
    "\n",
    "so first i am doing to do is import dataset from tensorflow_datasets anddivide into learning and testing ie 80 20\n",
    "\n",
    "and the create some usefull function like initiling first martic with np.randan create action function\n",
    "\n",
    "and the maths do back propagations mahi in hand then empliment it in coding\n",
    "\n",
    "and crea a loop to run evey thing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v1YMA31gC94q"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kkh5Vj8Y4k-2"
   },
   "source": [
    "# Initial SetUp\n",
    "\n",
    "## 1 Import all packages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "zv1WENZP46Z2",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0d8fc2e1-a5c0-4a75-d1e9-9a3df95ac749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_datasets in /usr/local/lib/python3.11/dist-packages (4.9.8)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.4.0)\n",
      "Requirement already satisfied: array_record>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.7.1)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.9)\n",
      "Requirement already satisfied: etils>=1.9.1 in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (1.12.2)\n",
      "Requirement already satisfied: immutabledict in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.2.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.0.2)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.29.3)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (5.9.5)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (18.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.32.3)\n",
      "Requirement already satisfied: simple_parsing in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (2.5.0)\n",
      "Requirement already satisfied: toml in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from tensorflow_datasets) (1.17.2)\n",
      "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (0.8.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (2025.3.0)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (6.5.2)\n",
      "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[edc,enp,epath,epy,etree]>=1.9.1; python_version >= \"3.11\"->tensorflow_datasets) (3.21.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow_datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->tensorflow_datasets) (2025.1.31)\n",
      "Requirement already satisfied: attrs>=18.2.0 in /usr/local/lib/python3.11/dist-packages (from dm-tree->tensorflow_datasets) (25.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from promise->tensorflow_datasets) (1.17.0)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from simple_parsing->tensorflow_datasets) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata->tensorflow_datasets) (1.69.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PchG112WDM3_"
   },
   "source": [
    "## 2 - Dataset\n",
    "\n",
    "I will be using the tensorflow_datasets library to load the MNIST dataset. This library provides access to MNIST data, but when I use tfds.load(), it returns the dataset as an object rather than a direct NumPy array. It also provides the total dataset length of Dataset ie 60000. I will convert the dataset into a NumPy array and split into two X and Y ,X be the image in  and Y being the lable.\n",
    "\n",
    "\n",
    "\n",
    "*   load dataset\n",
    "*   convert to nparray\n",
    "*   split the array to X and Y ,image and lable\n",
    "*   reshape the array from 28,28 to 782\n",
    "*   also normalize the X dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "WBJgurCv7HK0",
    "outputId": "f7bca5c6-1f89-418e-f4ce-73092a4e2746"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. shoud be 3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_datasets = tfds.load(\"mnist\",split='train').shuffle(10000).take(5000) #loading the test data using shuffle with buffor_size 10000\n",
    "test_datasets = tfds.load(\"mnist\",split='test').shuffle(10000).take(5000)\n",
    "# converting it to numpy array\n",
    "x_train=[]\n",
    "y_train=[]\n",
    "for i in tfds.as_numpy(train_datasets):\n",
    "  x_train.append(i['image'])\n",
    "  y_train.append(i['label'])\n",
    "\n",
    "\n",
    "x_train , y_train = np.array(x_train) , np.array(y_train)\n",
    "def ShowNumAndIMg():\n",
    "  n=np.random.randint(0,len(train_datasets))\n",
    "  plt.imshow(x_train[n])\n",
    "  print(f'no. shoud be {y_train[n]}')\n",
    "  return\n",
    "\n",
    "ShowNumAndIMg()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hK18gHXGuXYs"
   },
   "source": [
    "Reshaping the x_train from 28,28 to 784 and also  normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "No3tPkPkG6oV"
   },
   "outputs": [],
   "source": [
    "d,r,c,ch =x_train.shape\n",
    "# next code we are reshaping  the dataset\n",
    "x_train=x_train.reshape(d,r*c)\n",
    "#Normalizing\n",
    "x_train=np.divide(x_train,255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWWwRYd0RWOs"
   },
   "source": [
    "So dataset is now normalized aswell as in type that we can work on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NLgFpp4Rj0j"
   },
   "source": [
    "# 1.1 Vanilla Handwritten Digit Recognition Neural Network\n",
    "\n",
    "This section i will use just numpy and create a Neural Network.According to 3Blue1Brown he has NN of 784 input layner 10 output and 2 hidden layer with 16 Neural each on them.\n",
    "\n",
    "\n",
    "\n",
    "#### 1.   Funtion to initial weight and Basis\n",
    "\n",
    "In video he is not using He and Xavier initialization so i will be initialization with help of numpy.random.randn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "juhpNjkXbO8j",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c35503bf-1ca7-4ff7-fafb-dcc7254f7369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[-1.37780703, -0.23807009],\n",
      "       [ 2.25854978, -0.52685407],\n",
      "       [-0.05927726, -0.0184617 ],\n",
      "       [ 0.4126282 , -0.09529366],\n",
      "       [-0.51680108,  1.54029506],\n",
      "       [-0.29347896, -0.2518051 ],\n",
      "       [-0.15202714,  1.27580996],\n",
      "       [-0.9795014 , -0.85916377],\n",
      "       [ 0.09079154,  0.79128216],\n",
      "       [-0.73323349, -0.87227134],\n",
      "       [ 0.12572458,  1.73054083],\n",
      "       [ 0.365568  ,  1.71226296],\n",
      "       [-0.40127982,  0.01178727],\n",
      "       [-0.12077761,  1.12924984],\n",
      "       [ 0.90933794, -0.74621103],\n",
      "       [-0.75669288, -1.73639337]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[-1.73708114,  0.88897859, -0.35146195, -0.78951655,  1.85451997,\n",
      "         0.75152603, -1.37177922, -0.65778308,  0.87677429,  0.56158515,\n",
      "        -0.34781879, -0.04501667,  0.18474783, -0.2737815 , -0.13557926,\n",
      "        -0.56300929],\n",
      "       [-0.42491239,  0.69867437, -0.34424692,  0.19812094,  0.21950941,\n",
      "         0.46523553,  0.48310816,  0.20604768, -0.74675203, -0.1980174 ,\n",
      "        -0.87645098, -1.35718978, -0.59129383, -0.14498778, -0.1642733 ,\n",
      "         1.33652037],\n",
      "       [ 0.27321931,  0.24415331,  0.26735195, -0.7971504 ,  0.18718862,\n",
      "        -0.09632662, -1.60759001, -0.07831123, -0.7238866 ,  0.44554308,\n",
      "         0.07621117, -1.22407572, -0.87112293,  0.97824211, -0.0224609 ,\n",
      "        -0.09074654],\n",
      "       [ 0.76643528,  0.54162357,  1.23490262, -0.61492565,  0.32878618,\n",
      "        -1.80271149, -0.36030907,  1.33371415, -1.50073025,  0.26743309,\n",
      "        -0.11405931,  0.33517004, -0.3874801 ,  0.24943221, -1.7162688 ,\n",
      "         0.51835232],\n",
      "       [ 0.51603682,  0.6415663 , -0.82056256, -0.62590356, -1.74730483,\n",
      "         0.07355358,  0.14751263,  0.74036695,  0.33230453, -0.74346645,\n",
      "        -0.48530753,  2.06953423,  0.44046187, -0.37006665,  0.62240563,\n",
      "        -0.11154316],\n",
      "       [-1.15472832, -0.3800803 , -0.51673478,  1.05339583, -0.42495334,\n",
      "        -2.79376396, -0.99576768, -0.10075269, -0.51959274, -0.32371991,\n",
      "         1.02757113,  1.00272227, -0.93191027,  1.22863038, -1.31242577,\n",
      "        -0.81297165],\n",
      "       [ 0.26477458,  0.69138209,  0.93394617,  0.69177852,  0.40096923,\n",
      "        -0.16018546,  0.00813233, -2.58329648,  2.15779239,  0.47884597,\n",
      "        -0.88178632,  0.17744511,  1.16017776, -1.79876492,  0.70391327,\n",
      "         1.56860586],\n",
      "       [-0.4526849 , -0.16443825,  0.06823785, -0.21684923, -0.52418142,\n",
      "        -0.74226285,  0.35565121, -0.97612469,  0.14913665,  0.39277535,\n",
      "        -0.0897823 , -0.84148692,  0.3328767 ,  0.32569682,  3.01400182,\n",
      "         1.80931021],\n",
      "       [ 2.18181089, -0.36513514,  1.03794951, -0.45218645,  1.72914569,\n",
      "         0.18667751,  0.22032602, -1.29086563, -0.24425591, -0.58683114,\n",
      "        -0.44318834,  1.9013808 ,  0.22711863, -0.66129544,  0.34387836,\n",
      "         0.17108887],\n",
      "       [-0.15516374,  0.3085025 , -0.31615579,  0.62846433, -0.30078921,\n",
      "         0.52225733, -1.29289133,  0.68492653,  0.73472905, -0.58752559,\n",
      "        -0.23492672,  0.50812065,  0.52101423, -0.05778645, -2.11944003,\n",
      "         0.99648732],\n",
      "       [ 0.39986971,  0.64105044, -0.04795385, -0.41574376,  1.12628   ,\n",
      "        -1.46709052, -2.79277221,  0.26757061, -2.76430618,  0.09184736,\n",
      "        -0.69710954,  1.96894926,  0.43638994,  0.01400001,  1.10232633,\n",
      "         1.64664148],\n",
      "       [-0.20091609, -0.22871888, -0.16522538, -1.45185197, -0.70401779,\n",
      "         0.29262171,  0.51320657,  1.49133726, -0.57196329,  0.79807251,\n",
      "         0.50903232, -0.16456809, -0.86882111,  0.68851794,  0.4023916 ,\n",
      "         1.54844156],\n",
      "       [ 0.18679801, -0.7375942 ,  0.38688761, -0.42921805,  0.10457788,\n",
      "        -0.23133287, -0.15603852, -1.67205255,  1.62633904, -0.6572552 ,\n",
      "        -1.33356224, -0.93000145,  0.64685806, -0.84995704, -0.59498972,\n",
      "         0.82930854],\n",
      "       [ 0.08329193, -0.43515432,  0.65638781,  0.32165632,  1.9364922 ,\n",
      "         1.78715029,  0.68468433,  1.13358987,  0.54200829, -2.15784604,\n",
      "        -1.29039187, -0.39635192, -0.32669506,  0.42983022,  0.74042961,\n",
      "         0.01033398],\n",
      "       [ 0.52705065, -1.20142564,  1.84850008, -1.0657585 , -0.35785251,\n",
      "        -0.74887012,  1.03856577, -1.15190539, -0.19534245,  0.98550405,\n",
      "         0.5030586 , -0.60903617, -1.17114735, -0.23890742, -1.08515243,\n",
      "        -1.15721478],\n",
      "       [ 0.25006308, -0.57640739,  1.15067122, -0.92502064, -0.19641404,\n",
      "         0.12860204, -0.18038692, -0.00953801,  0.44463089, -0.03622323,\n",
      "        -1.31668756,  0.28135501,  1.35205478, -0.19307773, -0.63254435,\n",
      "         1.94581387]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[-1.71337637,  1.24002415,  1.63007329,  1.15483995, -0.79137858,\n",
      "         0.62571322,  1.46525362,  0.68123881,  0.87984557,  0.09239744,\n",
      "         2.04084782, -0.16381378,  2.53317174, -0.13362633, -0.76164088,\n",
      "         0.11614561],\n",
      "       [ 1.59296233, -0.48693013, -0.9700633 , -0.23697022,  1.43459297,\n",
      "         0.43804178,  0.3412601 ,  1.99593002, -0.58105858, -0.50941718,\n",
      "        -0.52487679,  0.55516608, -0.93950997, -0.88173105, -0.18864171,\n",
      "         1.23104716],\n",
      "       [-0.46208945, -0.1152827 , -0.31677297,  0.41113721, -0.94743675,\n",
      "         0.43724063,  0.026177  ,  1.36663358,  0.47645694,  1.0424952 ,\n",
      "         0.26495899, -1.06402026,  0.76306262, -0.1904292 ,  0.5430578 ,\n",
      "        -3.03911784],\n",
      "       [-1.43811464,  0.25074188, -1.07339295, -0.88178729, -1.0740912 ,\n",
      "        -0.95569983, -0.4186808 , -1.59452062,  0.49075404,  0.1996568 ,\n",
      "         0.3723744 , -1.3523958 , -2.00706469,  1.11710646, -1.14778932,\n",
      "        -0.39902837],\n",
      "       [-0.19208025, -0.70591368,  0.29131266,  1.50037602, -0.16760499,\n",
      "        -0.21480615,  0.10655115, -0.66621656, -0.05634404, -1.04922205,\n",
      "         0.37880262,  1.0715329 ,  0.94087546,  0.98166798, -1.87612162,\n",
      "         0.47183845],\n",
      "       [ 0.14339626,  1.00319379, -0.81504863,  1.49388577, -0.38078643,\n",
      "        -1.15121249, -0.0817157 ,  0.85630306,  1.09998063,  0.16138291,\n",
      "        -1.15566026, -1.02504031, -0.010926  , -0.22981146,  1.0110269 ,\n",
      "         1.851411  ],\n",
      "       [-1.69318426,  0.46693658, -1.30751502,  1.15135768,  0.46307971,\n",
      "        -1.79764493,  1.38547107, -1.51293297,  0.07383477, -0.21128531,\n",
      "        -0.51494693, -0.44563678, -0.60545044, -0.37368222,  0.41987059,\n",
      "        -1.08759069],\n",
      "       [ 1.4775512 ,  1.76080639,  0.89955286, -0.65582073, -0.03660751,\n",
      "         0.35166873,  1.29368337, -0.87402062, -0.31912275,  1.02387299,\n",
      "         0.20276534, -0.55424672, -0.57504968, -0.85902032,  0.51341639,\n",
      "         0.59815303],\n",
      "       [ 1.87849263, -0.1442963 ,  1.06663062, -1.25573148, -1.13006716,\n",
      "        -0.55709439, -0.31380454, -0.44599753, -1.22232111, -1.95361417,\n",
      "         0.13759188, -0.73581465,  0.36013632,  0.38466874,  0.21569526,\n",
      "        -0.51161587],\n",
      "       [-1.79412753, -1.59175506, -0.14504393,  1.99758021,  1.12734773,\n",
      "         0.46153224, -1.75148913,  0.82045478,  0.09755207, -0.42113341,\n",
      "        -0.36441155,  1.55165874,  0.97805505,  1.00342094, -0.79644994,\n",
      "         0.73854993]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]])}\n"
     ]
    }
   ],
   "source": [
    "def initial_values(layer_dims):\n",
    "  '''\n",
    "  this funtction should take in our case [784,16,16,10] where 784,is input layer but middle two layer 16,16 will have 16 Neurons and lat 10 output\n",
    "  so it shoud give\n",
    "  w1.shaep=(16,784)\n",
    "  w2.shape=(16,16)\n",
    "  w6.shape=(10,16)\n",
    "  '''\n",
    "  allmatrix = {}\n",
    "  for n in range(1,len(layer_dims)):\n",
    "    allmatrix['W' + str(n)]=np.random.randn(layer_dims[n],layer_dims[n-1])\n",
    "    allmatrix['b' + str(n)]=np.zeros((layer_dims[n], 1))\n",
    "\n",
    "  return allmatrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmeWt0cyk_Op"
   },
   "source": [
    "#### 2.  Activation Function\n",
    "\n",
    "in the video he did mention Leaky ReLU aswell as sigmod function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S1JRGaGURU7S"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    sigmoid =  np.divide(1,1+np.exp(-x) )\n",
    "    return sigmoid\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    relu =   np.maximum(0,x)\n",
    "    return relu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - The Training Loop #\n",
    "\n",
    "### 1.2.1   Forward Pass \n",
    "\n",
    "here function will take in input data and params in this case the math will be as follows:\n",
    "\n",
    "x->a1->a2->y\n",
    "\n",
    "$$z^{[1] (i)} =  W^{[1]} x^{(i)} + b^{[1] (i)}$$ \n",
    "$$a^{[1] (i)} =  ReLU(z^{[1] (i)})$$\n",
    "$$ $$\n",
    "$$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}$$\n",
    "$$a^{[2] (i)} = a^{[2] (i)} = Relu(z^{ [2] (i)})$$\n",
    "$$ $$\n",
    "$$z^{[3] (i)} = W^{[3]} a^{[2] (i)} + b^{[3] (i)}$$\n",
    "$$\\hat{y}^{(i)} = a^{[3] (i)} = \\sigma(z^{ [3] (i)})$$\n",
    "$$y^{(i)}_{prediction} = \\begin{cases} 1 & \\text{if } a^{[2](i)} > 0.5 \\\\ 0 & \\text{otherwise } \\end{cases}\\tag{5}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardPass(X,params):\n",
    "    '''\n",
    "    if params is ie if nn layers are:\n",
    "    2->2 will be len of params\n",
    "    3->4 will be len of params\n",
    "    4->6 will be len of params\n",
    "    5->8 will be len of params\n",
    "    and so on but 2 layer nn is not in our scope and from above i can make a equation:\n",
    "    len()/2+1 as rightside is just multiple of 2     \n",
    "    lets try this \n",
    "    '''\n",
    "    cache={}\n",
    "    layers=int(len(params) / 2)+1\n",
    "    \n",
    "    for i in range(1,layers):\n",
    "        if i == 1:\n",
    "            # first z1 with dot product of w1.x +b1\n",
    "            cache['Z'+str(i)]=np.dot(params[\"W\"+str(i)],X) + params[\"b\"+str(i)]\n",
    "            cache['A'+str(i)]=ReLU(cache['Z'+str(i)])\n",
    "        elif i == layers-1:\n",
    "            # last will sigmioid action vation for output layer \n",
    "            cache['Z'+str(i)]=np.dot(params[\"W\"+str(i)],cache['A'+str(i-1)]) + params[\"b\"+str(i)]\n",
    "            cache['A'+str(i)]=sigmoid(cache['Z'+str(i)])\n",
    "        else:\n",
    "            # for all hidden layers \n",
    "            cache['Z'+str(i)]=np.dot(params[\"W\"+str(i)],cache['A'+str(i-1)]) + params[\"b\"+str(i)]\n",
    "            cache['A'+str(i)]=ReLU(cache['Z'+str(i)])\n",
    "\n",
    "    assert(cache[\"A\"+str(layers-1)].shape == (10, X.shape[1]))\n",
    "    return cache[\"A\"+str(layers-1)], cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2  Computing the Cost\n",
    "\n",
    "sum of square of the diffrences between each of the output layer (Sum of Squared Errors) before that i need to create a function to change our y_train which is One Hot Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehotencoder(Y):\n",
    "    ohe = np.zeros((Y.size, Y.max() + 1))\n",
    "    ohe[np.arange(Y.size), Y] = 1\n",
    "    ohe = ohe.T\n",
    "    return ohe\n",
    "\n",
    "def costfunction(x,y):\n",
    "    y=onehotencoder(y)\n",
    "    length=y.shape(1)\n",
    "    cost=np.sum((x-y)**2)/length\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Backpropagation (Backward Pass)\n",
    "\n",
    "here we need to find how much  change shoud be made so that our cost becomes less or we need to find change a change delatc in function that poit to less or towards the out put we cant.we can achive this by calculas as we c our cost function has 3 main varialbe for last layer that will w3 b3 and a3 we can achive this by finding the dC with respect to dw3 ,dc with respect dta3 and dc with b3 applying chain rules and doing some maths we will get:\n",
    "\n",
    "for output and last hidden layer \n",
    "$$dZ^{[3]} = A^{[3]} - Y$$\n",
    "$$dW^{[3]} = \\frac{1}{m} dZ^{[3]} A^{[2]T}$$\n",
    "$$dB^{[3]} = \\frac{1}{m} \\Sigma {dZ^{[3]}}$$\n",
    "between hidden layers in our case that is 2\n",
    "$$dZ^{[2]} = W^{[3]T} dZ^{[3]} .* g^{[2]\\prime} (z^{[2]})$$\n",
    "$$dW^{[2]} = \\frac{1}{m} dZ^{[2]} A^{[1]T}$$\n",
    "$$dB^{[2]} = \\frac{1}{m} \\Sigma {dZ^{[2]}}$$\n",
    "and between input and first hidden layer \n",
    "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (z^{[1]})$$\n",
    "$$dW^{[1]} = \\frac{1}{m} dZ^{[1]} A^{[0]T}$$\n",
    "$$dB^{[1]} = \\frac{1}{m} \\Sigma {dZ^{[1]}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsig(x):\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    # m is just number of iteam or in or case pixel\n",
    "    m = X.shape[1]\n",
    "   # getiing the values of weight and activation we got from frwardpass from paras \n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    W3 = parameters[\"W3\"]  \n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    A3 = cache[\"A3\"]\n",
    "    # As our y_train is labled in numbers eg 8 here using the function crating a matrix \n",
    "    y=onehotencoder(Y)\n",
    "    # Backward propagation: calculate dW3, db3, dW2, db2, dW1, db1. \n",
    "    dZ3 = A3 - y\n",
    "    dW3 = np.dot(dZ3, A2.T) / m\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) / m\n",
    "\n",
    "    dZ2 = np.dot(W3.T, dZ3) * dsig(A2)\n",
    "    dW2 = np.dot(dZ2, A1.T) / m\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
    "    \n",
    "    dZ1 = np.dot(W2.T, dZ2) * dsig(A1)\n",
    "    dW1 = np.dot(dZ1, X.T) / m\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2,\n",
    "             \"dW3\": dW3,\n",
    "             \"db3\": db3,\n",
    "            }\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 - Update Parameters\n",
    "Function to upadte parama accoudring to gradiantdecent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate = 0.05):\n",
    "    # getting all wegihts and biases from parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    #  same for dwn dbn from grads \n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    dW3 = grads[\"dW3\"]\n",
    "    db3 = grads[\"db3\"]\n",
    "    # upating the paramets with learning rate \n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    W3 = W3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    # creating new para and returning it\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                 }\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - The Neural Network Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create loop as well initial the models parametrs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of flaten images\n",
    "    Y -- labels of image \n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    costs -- python list of the cost after 1000 iterations\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    np.random.seed(3)    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(10, 16,16,10)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    W3 = parameters[\"W3\"]\n",
    "    b3 = parameters[\"b3\"]\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A3, cache\".\n",
    "        A3, cache = forward_propagation(X, parameters)\n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A3, Y, parameters)\n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters, cache, X, Y)\n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Using our Model (Parameters) to make Predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Using the learned parameters, predicts a class for each example in X\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    X -- input data of size (n_x, m)\n",
    "    \n",
    "    Returns\n",
    "    predictions -- vector of predictions of our model (red: 0 / blue: 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    A3, cache = forward_propagation(X, parameters)\n",
    "    predictions = A3 > 0.5\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = nn_model(x_train, y_train, num_iterations = 10000, print_cost=True)\n",
    "\n",
    "plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)\n",
    "plt.title(\"Decision Boundary for hidden layer size \" + str(4))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
